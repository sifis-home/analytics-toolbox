# -*- coding: utf-8 -*-
"""Final of IBRL Dataset Anomaly Detection (Humidity).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y7GqxkdSW9u3ldPH3eI6sF5bBC6G-ncN
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from pandas import Series as sr, DataFrame as df
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow import keras
from tensorflow.keras import layers
#style.use('seaborn')
# %matplotlib inline

# from google.colab import drive
# drive.mount('drive')

# from google.colab import files
# uploaded = files.upload()

data = pd.read_csv("data.txt", sep = ' ', header = None)
print(data.head())

print(data.describe())

data.rename(columns = {0:'date',1:'time',2:'epochId',3:'moteId',4:'temp',5:'humidity',6:'light',7:'voltage'},inplace=True)

data = data.sort_values(['date','time'],ascending = [1,1])
print(data.head())

print(data["epochId"].unique())

for item in data["moteId"].unique():
  print(item)

for key,d in data.groupby("moteId"):
    print(key,"\n",d.head())
    print("--------------------------------")

sns.displot(data, x="humidity", binwidth=5)

import plotly.express as px
fig = px.histogram(data, x="humidity")
fig.show()

sns.displot(data, x="humidity", binwidth=100)

import plotly.express as px
fig = px.histogram(data, x="humidity")
fig.show()

d1=data[data["moteId"].isin([1])]

print(d1.head())
len(d1)
d1.to_csv('data_out.csv',index=False)
# !cp data_out.csv "drive/My Drive/"

print(d1.head())

# make sure date column is date as string dtype
d1['date'] = pd.to_datetime(d1['date']).dt.date.astype(str)

# make sure time column is of string dtype
d1['time'] = d1['time'].astype(str)

# combine date and time column (now both string dtype), then parse to datetime
d1['datetime'] = pd.to_datetime(d1['date'] + ' ' + d1['time'])

# df['datetime']
# 0   2012-10-16 09:38:08-05:00
# Name: datetime, dtype: datetime64[ns, pytz.FixedOffset(-300)]
print(d1.head())

ds=d1.groupby(['datetime']).mean().reset_index()
print(ds.head())
ds=ds.set_index(['datetime'])
print(ds.head())
print(d1.head())
#agg_10m = ds.groupby(pd.Grouper(freq='10Min')).aggregate(numpy.sum)

ds2=ds["humidity"]

print(ds2.head())

sns.relplot(data=ds2, kind="line", height = 8, aspect = 4)

first_filtered_dates=ds2.loc['2004-02-29':'2004-03-09']
first=ds2.loc['2004-03-11':'2004-03-14']
second=ds2.loc['2004-03-17':'2004-03-30']
second_filtered_dates=pd.concat([first, second])

print(first_filtered_dates.loc['2004-03-10':'2004-03-10'])

first_filtered_dates.drop(first_filtered_dates.loc['2004-03-10':'2004-03-10'].index, inplace=True)
second_filtered_dates.drop(second_filtered_dates.loc['2004-03-10':'2004-03-10'].index, inplace=True)

print(first_filtered_dates.loc['2004-03-10':'2004-03-10'])

print(second_filtered_dates.loc['2004-03-10':'2004-03-10'])

print(first_filtered_dates)

print(second_filtered_dates)

sns.relplot(data=first_filtered_dates, kind="line", height = 8, aspect = 4)

sns.relplot(data=second_filtered_dates, kind="line", height = 8, aspect = 4)

first_agg_5m = first_filtered_dates.groupby(pd.Grouper(freq='30Min')).aggregate(np.mean)

first_agg_5m.drop(first_agg_5m.loc['2004-03-10':'2004-03-10'].index, inplace=True)

print(first_agg_5m)

second_agg_5m = second_filtered_dates.groupby(pd.Grouper(freq='30Min')).aggregate(np.mean)

second_agg_5m.drop(second_agg_5m.loc['2004-03-15':'2004-03-16'].index, inplace=True)

print(second_agg_5m)

training_mean = first_agg_5m.mean()
training_std = first_agg_5m.std()
df_training_value = (first_agg_5m - training_mean) / training_std
#print(type(first_agg_5m))
#print(type(df_training_value))
#print(first_agg_5m.head())
#print(df_training_value.head())
print("Number of training samples:", len(df_training_value))
print(df_training_value.shape)

#print(first_agg_5m.shape)
#print(first_agg_5m.columns)
#print(first_agg_5m.keys())

print(training_mean)

print(training_std)

TIME_STEPS = 48

# Generated training sequences for use in the model.
def create_sequences(values, time_steps=TIME_STEPS):
    output = []
    for i in range(len(values) - time_steps + 1):
        output.append(values[i : (i + time_steps)])
    return np.stack(output)


x_train = create_sequences(df_training_value.values)
print("Training input shape: ", x_train.shape)

x_train=np.reshape(x_train,(433, 48, 1))

print(first_agg_5m.isnull().values.any())

check_nan_in_df = first_agg_5m.isnull()
print (check_nan_in_df)

check_nan_in_df.to_csv('check_nan_in_df.csv',index=True)
# !cp check_nan_in_df.csv "drive/My Drive/"

check_nan_in_df_sec = second_agg_5m.isnull()
print (check_nan_in_df_sec)

check_nan_in_df_sec.to_csv('check_nan_in_df_sec.csv',index=True)
# !cp check_nan_in_df_sec.csv "drive/My Drive/"

print(first_agg_5m.isnull().sum().sum())

print(second_agg_5m.isnull().sum().sum())

print(x_train[0])

model = keras.Sequential(
    [
        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),
        layers.Conv1D(
            filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Dropout(rate=0.2),
        layers.Conv1D(
            filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Conv1DTranspose(
            filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Dropout(rate=0.2),
        layers.Conv1DTranspose(
            filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
        ),
        layers.Conv1DTranspose(filters=1, kernel_size=7, padding="same"),
    ]
)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
print(model.summary())

history = model.fit(
    x_train,
    x_train,
    epochs=50,
    batch_size=64,
    validation_split=0.1,
    callbacks=[
        keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")
    ],
)

print(history.history)

sns.relplot(data=history.history, kind="line", height = 4, aspect = 4)

# Get train MAE loss.
x_train_pred = model.predict(x_train)
train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)

plt.hist(train_mae_loss, bins=50)
plt.xlabel("Train MAE loss")
plt.ylabel("No of samples")
plt.show()

# Get reconstruction loss threshold.
threshold = np.max(train_mae_loss)
print("Reconstruction error threshold: ", threshold)

# Checking how the first sequence is learnt
plt.plot(x_train[0])
plt.plot(x_train_pred[0])
plt.show()

df_test_value = (second_agg_5m - training_mean) / training_std
fig, ax = plt.subplots()
df_test_value.plot(legend=False, ax=ax)
plt.show()

#sns.relplot(data=df_test_value, kind="line", height = 8, aspect = 4)

# Create sequences from test values.
x_test = create_sequences(df_test_value.values)
print("Test input shape: ", x_test.shape)
x_test=np.reshape(x_test,(798, 48, 1))

# Get test MAE loss.
x_test_pred = model.predict(x_test)
test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)
test_mae_loss = test_mae_loss.reshape((-1))

plt.hist(test_mae_loss, bins=50)
plt.xlabel("test MAE loss")
plt.ylabel("No of samples")
plt.show()

# Detect all the samples which are anomalies.
anomalies = test_mae_loss > threshold
print("Number of anomaly samples: ", np.sum(anomalies))
print("Indices of anomaly samples: ", np.where(anomalies))

# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies
anomalous_data_indices = []
for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):
    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):
        anomalous_data_indices.append(data_idx)

df_subset = second_agg_5m.iloc[anomalous_data_indices]
fig, ax = plt.subplots()
second_agg_5m.plot(legend=False, ax=ax)
df_subset.plot(legend=False, ax=ax, color="r")
plt.show()

first_filtered_dates.to_csv('first_filtered_dates.csv',index=True)
!cp first_filtered_dates.csv "drive/My Drive/"

pd.DataFrame(second_filtered_dates).to_csv('second_filtered_dates.csv',index=True)
!cp second_filtered_dates.csv "drive/My Drive/"

